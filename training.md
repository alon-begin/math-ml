### Papers
* [NonProp](http://www-isl.stanford.edu/~widrow/papers/131.no_prop_neural_networks.pdf)
* [Synthetic Gradients](https://arxiv.org/pdf/1703.00522.pdf)
* [No More Pesky Learning Rates](https://arxiv.org/pdf/1206.1106.pdf)
* [Crossprop: Learning Representations by Stochastic
Meta-Gradient Descent in Neural Networks](https://arxiv.org/pdf/1612.02879.pdf)
* [Learning in the Machine: Random Backpropagation and the Learning Channel](https://arxiv.org/pdf/1612.02734.pdf)

### Blog posts
* [Learning without Backpropagation: Intuition and Ideas (Part 2)](http://www.breloff.com/no-backprop-part2/)
* [Expressivity, Trainability, and Generalization in Machine Learning](http://blog.evjang.com/2017/11/exp-train-gen.html?spref=tw)
* [Synthetic Gradients - explanation](https://iamtrask.github.io/2017/03/21/synthetic-gradients/)
* [YellowFin: An automatic tuner for momentum SGD](http://dawn.cs.stanford.edu/2017/07/05/yellowfin/)
* [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
* [Training](https://ml4a.github.io/ml4a/how_neural_networks_are_trained/)
